# Binominal and Normal Distribution

<aside>
📢 [Probability: Rules of Probability](https://www.codecademy.com/learn/probability-mssp/modules/rules-of-probability-mssp/cheatsheet)

</aside>

# *UNION, INTERSECTION, and COMPLEMENT*

> **UNION**
> 
- The *union* of two sets encompasses any element that exists in either one or both of them.

A = {1, 3, 5}    B = {3, 4, 5, 6}   

Union = {1, 3, 4, 5, 6}

- We can write the union of two events mathematically as ***(A or B)***.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/993841fe-0e9e-42ef-b7d7-278a6754a290/Untitled.png)

> **INTERSECTION**
> 
- The *intersection* of two sets encompasses any element that exists in both of the sets.

A = {1, 3, 5}    B = {3, 4, 5, 6}   

Intersection = {3, 5}

- We can write the intersection of two events mathematically as ***(A and B)***.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/49918916-bec5-494e-afee-23400a52b734/Untitled.png)

> **COMPLEMENT**
> 
- The *complement* of a set consists of all possible outcomes outside of the set.

A = {1, 3, 5}

Complement = {2, 4, 6}

- We can write the complement of set *A* as ***A**C**.*

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e67de603-55ac-4caa-a57c-e13c0090a09f/Untitled.png)

# *INDEPENDENCE AND DEPENDENCE*

- Two events are *independent* if the occurrence of one event does not affect the probability of the other.
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5da189d3-2786-4144-8cee-0950163dcb81/Untitled.png)
    
- Examples:
    - Suppose we roll a six-sided die two times. Event A is that we roll a 3 on the first roll. Event B is that we roll a 3 on the second roll. Are events A and B independent?
    
    >>> Independence
    
    - We pick out two cards from a standard deck of 52 cards without replacement. Event A is that we pick an Ace on the first draw. Event B is that we pick an Ace on the second draw. Are events A and B independent?
    
    >>> Dependence
    

# *MUTUALLY EXCLUSIVE EVENTS*

- Two events are considered *mutually exclusive* if they cannot occur at the same time.
    - For example, consider a single coin flip: the events “tails” and “heads” are mutually exclusive because we cannot get both tails and heads on a single flip.
- If event *A* is rolling an odd number and event *B* is rolling a number greater than two:
    - Not mutually exclusive. They have an intersection of *{3, 5}*.

# *ADDITION RULE*

> **IF ENVENTS ARE** *NOT* **MUTUALLY EXCLUSIVE**
> 
- We subtract the intersection of events *A* and *B* because it is included twice in the addition of *P(A)* and *P(B)*.

***P*(*A* or *B*)=*P*(*A*)+*P*(*B*)−*P*(*A* and *B*)**

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/53511a69-bfa9-407a-9cac-3ac5de115a20/Untitled.png)

> **IF EVENTS ARE MUTUALLY EXCLUSIVE**
> 
- The intersection is empty, so we don’t need to remove any overlap between the two events.

***P*(*A* or *B*)=*P*(*A*)+*P*(*B*)**

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7c8ef336-7612-4621-8bab-6f7189da4538/Untitled.png)

# *CONDITIONAL PROBABILITY*

- Conditional probability measures the probability of one event occurring, given that another one has already occurred.
- We denote the word “given” with a vertical line. |

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/45356ca9-456b-40ae-9f3a-7679ecddc380/Untitled.png)

> **DEPENDENT EVENTS**
> 
- If we pick two marbles from a bag of five marbles without replacement, the probability that the second marble is red depends on the color of the first marble.

**P(Red Second | Blue First) = 3/4**

> **INDEPENDENT EVENTS**
> 
- If we picked out two marbles with replacement, the probability of picking out a red marble or a blue marble second is unaffected by the first outcome.

**P(A | B) = P(A) and P(B | A) = P(B)**

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0c47213f-84f4-40ed-8737-4bb1271c51ab/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d3e0efb0-16f9-449b-b4b9-13f05e5f0ad5/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7d20ca09-7858-406c-9e53-d66ac8ccf877/Untitled.png)

# *MULTIPLICATION RULE*

- Calculate the probability that two events happen simultaneously.
- For two events, *A* and *B*, this is *P(A and B)* or the probability of the intersection of *A* and *B*.

> **GENERAL FORMULA for DEPENDENT EVENTS**
> 

**P(A and B) = P(A) . P(B | A)**

- We have five marbles: two are blue, and three are red. We pick two marbles without replacement.
- What if we want to know the probability of choosing a blue marble first AND a blue marble second?

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b3718b36-9841-4c03-97e1-85bbe6e23805/Untitled.png)

> **INDEPENDENT EVENTS**
> 

**P(A and B) = P(A) . P(B)**

- Flipping a fair coin twice. Event *A* is that we get tails on the first flip, and event *B* is that we get tails on the second flip.
- *P(A) = P(B) = 0.5*, so according to our formula, the probability of getting tails on both flips would be:

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8785678c-8f61-494d-a4cc-60050e6af299/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/60b2a071-ddeb-4937-b409-066c5670c1ef/Untitled.png)

# *BAYES’ THEOREM*

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/09ccd4e1-05cd-443e-8c31-7f379d6d5807/Untitled.png)

---

# *RANDOM VARIABLES random.choice(a, size = size, replace = True/False)*

In this lesson, we will use `random.choice(a, size = size, replace = True/False)` from the `numpy` library to simulate random variables in python. In this method:

- `a` is a list or other object that has values we are sampling from
- `size` is a number that represents how many values to choose
- `replace` can be equal to `True` or `False`, and determines whether we keep a value in `a` after drawing it (`replace = True`) or remove it from the pool (`replace = False`).

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/99a49fc1-36dc-49f0-a4c2-7c6e4a115016/Untitled.png)

Output:

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/65fe1851-3b92-4801-a76e-2225a4ecd869/Untitled.png)

# *DISCRETE and CONTINUOUS RANDOM VARIABLES*

> **DISCRETE RANDOM VARIABLES**
> 
- Random variables with ***a countable number of possible values*** are called ***discrete random variables.***
- Example:
    - Rolling a regular 6-sided die would be considered a discrete random variable because the outcome options are limited to the numbers on the die.
    - When observing counting events, such as how many people entered a store on a randomly selected day.
    - The individual pieces of candy a randomly selected child receives on a candy-giving holiday.
    

> **CONTINUOUS RANDOM VARIABLES**
> 
- When the ***possible values of a random variable are uncountable***, it is called ***a continuous random variable.*** These are generally measurement variables and are uncountable because measurements can always be more precise – meters, centimeters, millimeters, etc.
- Example:
    - The temperature in Los Angeles on a randomly chosen day is a continuous random variable (96 degrees, 96.44 degrees, 96.437 degrees, etc.).
    - The time it takes a randomly selected person to run a mile.
    

# *PROBABILITY MASS FUNCTIONS (PMF)*

Suppose that we flip a fair coin some number of times and count the number of heads. The probability mass function that describes the likelihood of each possible outcome (eg., 0 heads, 1 head, 2 heads, etc.) is called the *binomial distribution*. The parameters for the binomial distribution are:

- `n` for the number of trials (eg., n=10 if we flip a coin 10 times)
- `p` for the probability of success in each trial (probability of observing a particular outcome in each trial. In this example, p= 0.5 because the probability of observing heads on a fair coin flip is 0.5)

If we flip a fair coin 10 times, we say that the number of observed heads follows a `Binomial(n=10, p=0.5)` distribution. The graph below shows the probability mass function for this experiment. The heights of the bars represent the probability of observing each possible outcome as calculated by the PMF.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/76840366-6703-4af3-8642-e077f705c2df/Untitled.png)

Example from Chat GPT: 

- Each side of the die has a different number: 1, 2, 3, 4, 5, or 6. Now, let's say you want to know the chances of getting each number when you roll the die.
    - The chances of getting a 1 are 1/6 (because there's only one side with a 1).
    - The chances of getting a 2 are 1/6 (because there's only one side with a 2).
    - The chances of getting a 3 are 1/6 (because there's only one side with a 3).
    - The chances of getting a 4 are 1/6 (because there's only one side with a 4).
    - The chances of getting a 5 are 1/6 (because there's only one side with a 5).
    - The chances of getting a 6 are 1/6 (because there's only one side with a 6).
- Chances of getting a number less than or equal to 3 = 1/6 + 1/6 + 1/6 = 3/6 = 1/2.
- So, the chances of rolling a number less than or equal to 3 on our special die is 1/2 or 50%.

# *CALCULATING PROBABILITIES USING PYTHON* binom.pmf(x, n, p) from scipy.stats library

Used to calculate the PMF of the binomial distribution at any value. This method takes 3 values:

- `x`: the value of interest
- `n`: the number of trials
- `p`: the probability of success

For example, suppose we flip a fair coin 10 times and count the number of heads. 

We can use the `binom.pmf()` function to calculate the probability of observing 6 heads as follows:

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4f6222b2-eb89-41ec-9a46-700c13740ff1/Untitled.png)

Output: # 0.205078

# *PROBABILITY MASS FUNCTION OVER A RANGE USING PYTHON*

- For example, we can calculate the probability of observing between 2 and 4 heads from 10-coin flips as follows:

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/aebe4e91-1377-4599-9bbd-976c7085553d/Untitled.png)

Output: # 0.366211
``

- We can also calculate the probability of observing less than a certain value, let’s say 3 heads, by adding up the probabilities of the values below it:

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d04b4597-367c-43ef-be67-c77481e46785/Untitled.png)

Output: # 0.0546875

- If we want to know the probability of observing 8 or fewer heads from 10 coin flips:

**P(0 to 8 heads) + P(9 to 10 heads) = P(0 to 10 heads) = 1**

**P(0 to 8 heads) = 1 - P(9 to 10 heads)**

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ebc29317-1530-4c74-a796-58317602e60b/Untitled.png)

Output: # 0.98926

# *CUMULATIVE DISTRIBUTION FUNCTION (CDF)*

- Instead of the probability of observing a specific value, the cumulative distribution function gives the probability of observing a specific value OR LESS.
- The probabilities for all possible values in a given probability distribution add up to 1. The value of a cumulative distribution function at a given value is equal to the sum of the probabilities lower than it, with a value of 1 for the largest possible number.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2579adeb-0724-4c9d-bf07-1aecfc5bc64b/Untitled.png)

- We showed how the probability mass function can be used to calculate the probability of observing less than 3 heads out of 10 coin flips by adding up the probabilities of observing 0, 1, and 2 heads. The cumulative distribution function produces the same answer by evaluating the function at `CDF(X=2)`. In this case, using the CDF is simpler than the PMF because it requires one calculation rather than three.
- The animation to the right shows the relationship between the probability mass function and the cumulative distribution function.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dd147720-bd6b-468d-8c51-95235c3db713/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2dc4c128-13ca-4e85-b7ab-4400f6d812c6/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f891ba27-ad30-484b-8289-30a7fbfa6bef/Untitled.png)

> **CUMULATIVE DISTRIBUTION FUNCTION CONTINUED**
> 
- We can use a cumulative distribution function to calculate the probability of a specific range by taking the difference between two values from the cumulative distribution function.
- For example, to find the probability of observing between 3 and 6 heads, we can take the probability of observing 6 or fewer heads and subtracting the probability of observing 2 or fewer heads.
- This leaves a remnant of between 3 and 6 heads.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6e0adaeb-eaf4-4c8f-ae32-ba3d9609ef47/Untitled.png)

# *USING THE CUMULATIVE DISTRIBUTION FUNCTION IN PYTHON* binom.cdf(x, n, p) from scipy.stats library

> **Basically a shortened and more sufficient version of PMF**
> 
- This method takes 3 values:
    - `x`: the value of interest, looking for the probability of this value or less
    - `n`: the sample size
    - `p`: the probability of success

- Calculating the probability of observing **6 or fewer heads** from 10 fair coin flips (0 to 6 heads):

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8af21055-ae2f-404c-baec-4f3eaacb6a38/Untitled.png)

**P(6 or fewer heads) = P(0 to 6 heads)**

Output: 0.828125

- Calculating the probability of observing **between 4 and 8 heads** from 10 fair coin flips can be thought of as taking the difference of the value of the cumulative distribution function at 8 from the cumulative distribution function at 3:

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c2414783-88e4-4bb8-8b99-c4d80d095632/Untitled.png)

**P(4 to 8 heads) = P(0 to 8 heads) - P(0 to 3 heads)**

Output: 0.81738

- Calculate the probability of observing **more than 6 heads** from 10 fair coin flips we subtract the value of the cumulative distribution function at 6 from 1:
- Note that ***“more than 6 heads” does not include 6***.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/80445c69-83eb-42d1-8011-ed63cce63461/Untitled.png)

**P(more than 6 heads) = 1 - P(6 or fewer heads)**

Output: 0.171875

# ***PROBABILITY DENSITY FUNCTIONS* stats.norm.cdf(x, loc, scale)**

- Similar to how discrete random variables relate to probability mass functions, continuous random variables relate to probability density functions.
- They define the probability distributions of continuous random variables and span across all possible values that the given random variable can take on.
- In a probability density function, we cannot calculate the probability at a single point.
- This is because the area of the curve underneath a single point is always zero.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/83cc195f-0566-4fe0-a0e7-aef90543595d/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/700d809c-3800-4f47-b46b-49da9377932b/Untitled.png)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e7cdc169-795e-4ef6-98ef-a1191bbc3924/Untitled.png)

- As the interval becomes smaller, the width of the area under the curve becomes smaller as well.
- When trying to evaluate the area under the curve at a specific point, the width of that area becomes 0, and therefore the probability equals 0.

> **Example**
> 
- Heights fall under a type of probability distribution called a *normal distribution*. The parameters for the normal distribution are the mean and the standard deviation, and we use the form *Normal(mean, standard deviation)* as shorthand.
- Women’s heights have a mean of 167.64 cm with a standard deviation of 8 cm >>> Normal(167.64, 8)
- Let’s say we want to know the probability that a randomly chosen woman is less than 158 cm tall.
- We can use the cumulative distribution function to calculate the area under the probability density function curve from 0 to 158 to find that probability.

We can calculate the area of the blue region in Python using the `norm.cdf()` method from the `scipy.stats` library. This method takes on 3 values:

- `x`: the value of interest
- `loc`: the mean of the probability distribution
- `scale`: the standard deviation of the probability distribution

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c3555d47-6f17-4a41-b5e2-3a2fabc866b5/Untitled.png)

Output: 0.1141

# *PROBABILITY DENSITY FUNCTIONS AND CUMULATIVE DISTRIBUTION FUNCTION*

- Calculating the probability of randomly observing a woman between 165 cm to 175 cm, assuming heights still follow the Normal(167.74, 8) distribution.
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b940c2db-16b0-40ea-b56c-c75d87213789/Untitled.png)
    
    Output: 0.45194
    
- Calculate the probability of observing a woman taller than 172 centimeters, assuming heights still follow the Normal(167.74, 8) distribution.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d59d5f83-8fb1-4ae3-9777-b1aceb65b4e7/Untitled.png)

Output: 0.29718
